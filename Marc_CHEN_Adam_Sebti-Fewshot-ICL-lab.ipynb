{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f6c0e94e80fa42a183c4de48c0b0c889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8c7de4041174df6ac161e4e13828af4",
              "IPY_MODEL_3921dd85dd644f089de54459b47741d9",
              "IPY_MODEL_d2694c9372c7486e854e63dfdb864b91"
            ],
            "layout": "IPY_MODEL_82e21e30d59946c7aaa916205a798cf1"
          }
        },
        "e8c7de4041174df6ac161e4e13828af4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df64e4a54cf34714ab70719c9eb1b821",
            "placeholder": "​",
            "style": "IPY_MODEL_da1314a036574b558c3d4e1633f4a1c3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3921dd85dd644f089de54459b47741d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3f0cbdd50fc4ef1816ba814845110c9",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f82fa41a1b549d69858c2fa73e8fb48",
            "value": 4
          }
        },
        "d2694c9372c7486e854e63dfdb864b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_490702f69ab248678312f0bb2fe90ca1",
            "placeholder": "​",
            "style": "IPY_MODEL_aebac4525d7a4432bb0a450fa9ec8aec",
            "value": " 4/4 [01:26&lt;00:00, 18.41s/it]"
          }
        },
        "82e21e30d59946c7aaa916205a798cf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df64e4a54cf34714ab70719c9eb1b821": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da1314a036574b558c3d4e1633f4a1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3f0cbdd50fc4ef1816ba814845110c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f82fa41a1b549d69858c2fa73e8fb48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "490702f69ab248678312f0bb2fe90ca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aebac4525d7a4432bb0a450fa9ec8aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 3: Fewshot ICL\n",
        "\n",
        "As knowledge graph requires background in SPARQL and/or LLM finetuning, this lab won't be totally related to what you saw in today's course.\n",
        "\n",
        "We'll be delving into In Context Learning (ICL), in particular ICL fewshot, and trying to understand how it works and when to use it. To do this, we'll be using the Transformer Library, a Mistral LLM and an emotion classification dataset.\n",
        "\n",
        "\n",
        "The laboratory is divided into 4 sections:\n",
        "0. Setup: This section is dedicated to installing modules, loading models and loading data.You don't need to code, just run it.\n",
        "1. Zeroshot Classification: Some of you may have had trouble finding a prompt that always returned a “well-formed” answer in the last lab. In this section, we'll use a “well-formed” prompt to perform zeroshot classification.\n",
        "2. Fewshot Classification - Random Retrieval: One of the most common methods of improving ICL classification is to add demonstrations to the prompt. This helps the LLM to “properly format” the response and can also give semantic information about how to solve the task. In this section, we will use random retrieved demonstration and compare the results with those of section 1.\n",
        "3. Fewshot Classification - Vector-based Retrieval: Extracting random demonstrations in fewshot classification can introduce bias. In addition, most semantically relevant demonstrations are not taken into account. As with did with RAG, we will use a vector representation of the example to retrieve the most relevant demonstrations.\n",
        "4. Constrained Decoding: Finally, we'll discovering the `outlines` library, which contains modules that are useful to do constrained decoding.\n",
        "\n",
        "At the end of each section (except section 0.), there's a question to answer."
      ],
      "metadata": {
        "id": "claAHmTTZe9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Setup"
      ],
      "metadata": {
        "id": "5YVY9aJOg9bU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tINI3qJvIQIE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers bitsandbytes accelerate datasets outlines scikit-learn > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "Nl1k2J6aW69V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GenerationConfig\n",
        ")\n",
        "\n",
        "import torch\n",
        "\n",
        "# Put your hugging face token here: https://huggingface.co/docs/hub/en/security-tokens\n",
        "# You need to fill the access form with your huggingface account on this link: https://huggingface.co/mistralai/Ministral-8B-Instruct-2410\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "llm_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
        "\n",
        "# We want to use 4bit quantization to save memory\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=False, load_in_4bit=True\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_name, padding_side=\"left\", token=hf_token)\n",
        "# Prevent some transformers specific issues.\n",
        "tokenizer.use_default_system_prompt = False\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load LLM.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map={\"\": 0}, # load all the model layers on GPU 0\n",
        "    torch_dtype=torch.bfloat16, # float precision\n",
        "    token=hf_token\n",
        ")\n",
        "# Set LLM on eval mode.\n",
        "llm.eval()\n"
      ],
      "metadata": {
        "id": "gOxlDh8hQZGK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551,
          "referenced_widgets": [
            "f6c0e94e80fa42a183c4de48c0b0c889",
            "e8c7de4041174df6ac161e4e13828af4",
            "3921dd85dd644f089de54459b47741d9",
            "d2694c9372c7486e854e63dfdb864b91",
            "82e21e30d59946c7aaa916205a798cf1",
            "df64e4a54cf34714ab70719c9eb1b821",
            "da1314a036574b558c3d4e1633f4a1c3",
            "e3f0cbdd50fc4ef1816ba814845110c9",
            "7f82fa41a1b549d69858c2fa73e8fb48",
            "490702f69ab248678312f0bb2fe90ca1",
            "aebac4525d7a4432bb0a450fa9ec8aec"
          ]
        },
        "outputId": "aafc9eec-694c-47a0-de62-8d5c24210028"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6c0e94e80fa42a183c4de48c0b0c889"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(131072, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-35): 36 x MistralDecoderLayer(\n",
              "        (self_attn): MistralSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): MistralRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=131072, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up our generation configuration.\n",
        "# We set max_new_token to 128 to reduce computation time (we may also lose some accuracy).\n",
        "# We disable beamsearch to ensure reproducibility (we may lose some accuracy).\n",
        "generation_config = GenerationConfig(\n",
        "  max_new_tokens = 128,\n",
        "  do_sample=False,\n",
        "  eos_token_id=tokenizer.eos_token_id,\n",
        "  pad_token_id=tokenizer.pad_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "a93dg8OfWM6V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "id2label = {0:\"sadness\", 1:\"joy\", 2:\"love\", 3:\"anger\", 4:\"fear\", 5:\"surprise\"}\n",
        "\n",
        "\n",
        "# Dataset: https://huggingface.co/datasets/dair-ai/emotion\n",
        "ds = load_dataset(\"dair-ai/emotion\", \"split\")\n",
        "examples = [{\"text\":ex[\"text\"], \"label\":id2label[ex[\"label\"]]}for ex in ds['test'].to_list()]\n",
        "random.shuffle(examples)\n",
        "\n",
        "# Split examples and keep only a few samples to have short computation time.\n",
        "test, train = examples[:100], examples[100:500]\n",
        "print(f\"Train len {len(train)}. Test len {len(test)}\")\n",
        "print(f\"First example of test:\\n{test[0]}\")"
      ],
      "metadata": {
        "id": "0z6wXOAGNi-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20406eaf-1cba-460c-9162-c9ebe8e9aba9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train len 400. Test len 100\n",
            "First example of test:\n",
            "{'text': 'i feel a strange gratitude for the hated israeli occupation of sinai that lasted from to for actually recognizing the importance of sinais history', 'label': 'surprise'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Zero-shot Classification\n",
        "\n",
        "It's very similar to what you've done last time, so we're providing you with most of the code. The only thing you need to code yourself is the parse_answer function.\n",
        "- We adapted the recommended classification prompt from: https://docs.mistral.ai/guides/prompting_capabilities/\n",
        "- The purpose of this function is to return the first occurrence of a correct label (sadness, joy, love, anger, fear, surprise)\n",
        "- We want to return \"\" if no answer is found.\n",
        "- You can use regex or string functions.\n",
        "\n",
        "There is a cell below to test your code. The output should be:\n",
        "\n",
        "```\n",
        "##### Example 0 #####\n",
        "# You're an expert in sentiment analysis. Your task is to classify the sentence emotion after <<<>>> with one of the following predefined labels:\n",
        "\n",
        "sadness\n",
        "joy\n",
        "love\n",
        "anger\n",
        "fear\n",
        "surprise\n",
        "\n",
        "You will only respond with the category. Do not include the word \"Category\". Do not provide explanations or notes.\n",
        "\n",
        "<<<\n",
        "Sentence: i feel a strange gratitude for the hated israeli occupation of sinai that lasted from to for actually recognizing the importance of sinais history\n",
        "Label:\n",
        ">>>\n",
        "# sadness\n",
        "# sadness\n",
        "```"
      ],
      "metadata": {
        "id": "BbxicYCqXFoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "zeroshot_prompt = \"\"\"\n",
        "You're an expert in sentiment analysis. Your task is to classify the sentence emotion after <<<>>> with one of the following predefined labels:\n",
        "\n",
        "sadness\n",
        "joy\n",
        "love\n",
        "anger\n",
        "fear\n",
        "surprise\n",
        "\n",
        "You will only respond with the label. Do not include the word \"Label\". Do not provide explanations or notes.\n",
        "\n",
        "<<<\n",
        "Sentence: {sentence}\n",
        "Label:\n",
        ">>>\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def generate(prompt, llm=llm, generation_config=generation_config):\n",
        "\n",
        "  # Create turns with the given prompt\n",
        "  turns = [\n",
        "    {'role':'user', 'content':prompt}\n",
        "  ]\n",
        "\n",
        "  # Tokenize turns.\n",
        "  input_ids = tokenizer.apply_chat_template(turns, return_tensors='pt').to('cuda')\n",
        "\n",
        "  # Ensure we don't use gradient to save memory space and computation time.\n",
        "  with torch.no_grad():\n",
        "    outputs = llm.generate(\n",
        "      input_ids,\n",
        "      generation_config\n",
        "    )\n",
        "\n",
        "  # Recover and decode answer.\n",
        "  answer_tokens = outputs[0, input_ids.shape[1]:-1]\n",
        "  return tokenizer.decode(answer_tokens).strip()\n",
        "\n",
        "\n",
        "def parse_answer(answer):\n",
        "  # Remove any extraneous text around the label\n",
        "  label = answer.strip().split(\"\\n\")[0]\n",
        "  return label"
      ],
      "metadata": {
        "id": "HwNIAD_uT53U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now apply the fewshot prompt on the full test dataset. You need report:\n",
        "- Accuracy (recall: number of correct answers divided by number of samples)\n",
        "- Ratio of missing answer (i.e \".\" answer)\n",
        "\n",
        "It should take 3 to 5 minutes to run."
      ],
      "metadata": {
        "id": "CLcsV6Rys-tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "results = []\n",
        "correct_predictions = 0\n",
        "missing_answers = 0\n",
        "total_examples = len(test)\n",
        "\n",
        "for example in tqdm(test):\n",
        "    prompt = zeroshot_prompt.format(sentence=example[\"text\"])\n",
        "    answer = generate(prompt)\n",
        "    prediction = parse_answer(answer)\n",
        "\n",
        "    results.append({\n",
        "        \"text\": example[\"text\"],\n",
        "        \"label\": example[\"label\"],\n",
        "        \"prediction\": prediction\n",
        "    })\n",
        "\n",
        "    if prediction == example[\"label\"]:\n",
        "        correct_predictions += 1\n",
        "    if prediction == \"\" or prediction not in [\"sadness\",\"joy\",\"love\",\"anger\",\"fear\",\"surprise\"]:\n",
        "        missing_answers += 1\n",
        "\n",
        "accuracy = correct_predictions / total_examples\n",
        "missing_answer_ratio = missing_answers / total_examples\n",
        "\n",
        "# Add statistics to the results\n",
        "results.append({\n",
        "    \"statistics\": {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"missing_answer_ratio\": missing_answer_ratio\n",
        "    }\n",
        "})\n",
        "\n",
        "# Save the results to a JSON file\n",
        "with open(\"Zero_shot_Classification.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Missing answer ratio: {missing_answer_ratio}\")\n",
        "print(\"Results saved to Zero_shot_Classification.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koANDPXkaX2D",
        "outputId": "4bc99c9b-35cb-4562-9581-77b269c60741"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:49<00:00,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.63\n",
            "Missing answer ratio: 0.0\n",
            "Results saved to Zero_shot_Classification.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: We always find an answer, because we've used a “well-formed” prompt and because Mistral is good at following this type of instruction. If you try with the Lama-3, some answers may be missing.\n",
        "\n",
        "**Question: Are we sure that all these answer are \"well-formed\" answer ?**"
      ],
      "metadata": {
        "id": "QnJCGcVQh7FU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this prompt and the use of Mistral, we can be confident that the answers are either an empty string (\"\") or one of the predefined labels: \"sadness,\" \"joy,\" \"love,\" \"anger,\" \"fear,\" or \"surprise,\" making them well-formed. However, there is a possibility that the model might hallucinate and generate an answer not included in the provided list."
      ],
      "metadata": {
        "id": "izVhqA-acH32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fewshot Classification - Random Retrieval:\n",
        "\n",
        "Now we have a working zeroshot solution. Our next next step is to use demonstrations. We will start be implementing a random few shot generation. You need to implement 3 functions:\n",
        "\n",
        "- format_demo, wich format a given example into a demonstration string\n",
        "- format_demos, wich format a given list of example into a demonstration string (try to use format_demo)\n",
        "- get_random_demo, wich return k random examples. (you should use random.choice. https://docs.python.org/3/library/random.html)\n",
        "\n",
        "\n",
        "There is a cell below to test your code. The output should be:\n",
        "```\n",
        "##### format_demo #####\n",
        "# Sentence: i feel a strange gratitude for the hated israeli occupation of sinai that lasted from to for actually recognizing the importance of sinais history\n",
        "Label: surprise.\n",
        "\n",
        "\n",
        "##### format_demos #####\n",
        "# Sentence: i feel a strange gratitude for the hated israeli occupation of sinai that lasted from to for actually recognizing the importance of sinais history\n",
        "Label: surprise.\n",
        "\n",
        "Sentence: im feeling optimistic to finish out these last two weeks strong and probably continue with what i have been doing\n",
        "Label: joy.\n",
        "\n",
        "Sentence: i feel complacent and satisfied\n",
        "Label: joy.\n",
        "\n",
        "Sentence: im the only one with all the feelings and emotions and thats just pathetic of me to do so\n",
        "Label: sadness.\n",
        "\n",
        "Sentence: i just sat there in my group feeling really depressed because my book just had to go missing at this time\n",
        "Label: sadness.\n",
        "\n",
        "\n",
        "##### Example 0 #####\n",
        "# You're an expert in sentiment analysis. Your task is to classify the sentence emotion after <<<>>> with one of the following predefined labels:\n",
        "\n",
        "sadness\n",
        "joy\n",
        "love\n",
        "anger\n",
        "fear\n",
        "surprise\n",
        "\n",
        "You will only respond with the label. Do not include the word \"Label\". Do not provide explanations or notes.\n",
        "\n",
        "####\n",
        "Here are some examples:\n",
        "\n",
        "Sentence: i feel inspired so many thing i want to write down\n",
        "Label: joy.\n",
        "\n",
        "Sentence: i feel like i should have some sort of rockstar razzle dazzle lifestyle but i would at least like to spend a third of my life doing something i feel is worthwhile\n",
        "Label: joy.\n",
        "\n",
        "Sentence: i continue to write this i feel more and more distraught\n",
        "Label: fear.\n",
        "\n",
        "Sentence: i feel that third situation pretty much sums up my feelings toward this title\n",
        "Label: joy.\n",
        "\n",
        "Sentence: i remember wanting to fit in so bad and feeling like no one liked me\n",
        "Label: love.\n",
        "####\n",
        "\n",
        "<<<\n",
        "Sentence: i feel a strange gratitude for the hated israeli occupation of sinai that lasted from to for actually recognizing the importance of sinais history\n",
        "Label:\n",
        ">>>\n",
        "# sadness\n",
        "# sadness\n",
        "```"
      ],
      "metadata": {
        "id": "TNnwc7gXeC23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fewshot_prompt = \"\"\"\n",
        "You're an expert in sentiment analysis. Your task is to classify the sentence emotion after <<<>>> with one of the following predefined labels:\n",
        "\n",
        "sadness\n",
        "joy\n",
        "love\n",
        "anger\n",
        "fear\n",
        "surprise\n",
        "\n",
        "You will only respond with the label. Do not include the word \"Label\". Do not provide explanations or notes.\n",
        "\n",
        "####\n",
        "Here are some examples:\n",
        "\n",
        "{examples}\n",
        "####\n",
        "\n",
        "<<<\n",
        "Sentence: {sentence}\n",
        "Label:\n",
        ">>>\n",
        "\"\"\".strip()\n",
        "\n",
        "def format_demo(demo):\n",
        "  demonstration_string = f\"Sentence: {demo['text']}\\nLabel: {demo['label']}.\"\n",
        "  return demonstration_string\n",
        "\n",
        "def format_demos(demos):\n",
        "  format_demos  = \"\"\n",
        "  for demo in demos:\n",
        "    format_demos += format_demo(demo) + \"\\n\\n\"\n",
        "  return format_demos\n",
        "\n",
        "def get_random_demo(k, train=train):\n",
        "  random_demos = random.choices(train, k=k)\n",
        "  return random_demos\n"
      ],
      "metadata": {
        "id": "-bJZMJVeZm-G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now apply the fewshot prompt on the full test dataset. You need report:\n",
        "- Accuracy (recall: number of correct answers divided by number of samples)\n",
        "- Report them for k=1 and k=5\n",
        "\n",
        "It should take 5 to 7 minutes to run.\n",
        "\n"
      ],
      "metadata": {
        "id": "8E1Hr-cVF3pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "K=1\n",
        "random.seed(42)\n",
        "\n",
        "results = []\n",
        "correct_predictions = 0\n",
        "missing_answers = 0\n",
        "total_examples = len(test)\n",
        "\n",
        "for example in tqdm(test):\n",
        "\n",
        "    demos = format_demos(get_random_demo(K))\n",
        "\n",
        "    prompt = fewshot_prompt.format(examples=demos, sentence=example[\"text\"])\n",
        "    answer = generate(prompt)\n",
        "    prediction = parse_answer(answer)\n",
        "\n",
        "    results.append({\n",
        "        \"text\": example[\"text\"],\n",
        "        \"label\": example[\"label\"],\n",
        "        \"prediction\": prediction\n",
        "    })\n",
        "\n",
        "    if prediction == example[\"label\"]:\n",
        "        correct_predictions += 1\n",
        "    if prediction == \"\":\n",
        "        missing_answers += 1\n",
        "\n",
        "accuracy = correct_predictions / total_examples\n",
        "missing_answer_ratio = missing_answers / total_examples\n",
        "\n",
        "# Add statistics to the results\n",
        "results.append({\n",
        "    \"statistics\": {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"missing_answer_ratio\": missing_answer_ratio\n",
        "    }\n",
        "})\n",
        "\n",
        "with open(\"Few_shot_Classification_K1.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Missing answer ratio: {missing_answer_ratio}\")\n",
        "print(\"Results saved to Few_shot_Classification_K1.json\")"
      ],
      "metadata": {
        "id": "bEAbpeiLu9IW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b0c7c4-b6c7-4a7b-b7cc-c15a891670bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [02:11<00:00,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6\n",
            "Missing answer ratio: 0.0\n",
            "Results saved to Few_shot_Classification_K1.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "K=5\n",
        "random.seed(42)\n",
        "\n",
        "results = []\n",
        "correct_predictions = 0\n",
        "missing_answers = 0\n",
        "total_examples = len(test)\n",
        "\n",
        "for example in tqdm(test):\n",
        "\n",
        "    demos = format_demos(get_random_demo(K))\n",
        "\n",
        "    prompt = fewshot_prompt.format(examples=demos, sentence=example[\"text\"])\n",
        "    answer = generate(prompt)\n",
        "    prediction = parse_answer(answer)\n",
        "\n",
        "    results.append({\n",
        "        \"text\": example[\"text\"],\n",
        "        \"label\": example[\"label\"],\n",
        "        \"prediction\": prediction\n",
        "    })\n",
        "\n",
        "    if prediction == example[\"label\"]:\n",
        "        correct_predictions += 1\n",
        "    if prediction == \"\":\n",
        "        missing_answers += 1\n",
        "\n",
        "accuracy = correct_predictions / total_examples\n",
        "missing_answer_ratio = missing_answers / total_examples\n",
        "\n",
        "# Add statistics to the results\n",
        "results.append({\n",
        "    \"statistics\": {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"missing_answer_ratio\": missing_answer_ratio\n",
        "    }\n",
        "})\n",
        "\n",
        "with open(\"Few_shot_Classification_K5.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Missing answer ratio: {missing_answer_ratio}\")\n",
        "print(\"Results saved to Few_shot_Classification_K5.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHlUg_pUwoT9",
        "outputId": "ea0e1c7d-d977-42e0-8623-00c673ba1b85"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [03:13<00:00,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.65\n",
            "Missing answer ratio: 0.0\n",
            "Results saved to Few_shot_Classification_K5.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question: What are the limits of using a single demonstration? What are the limits of using too many demonstrations?**"
      ],
      "metadata": {
        "id": "X_F1U0jMnDhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Using a single representation has several limitations. Using a limited number of representation might not adequally represent the diverity of the task leading to poor generalization. More over this could lead to bias and noise.\n",
        "- Using too many demonstrations increases the computaional cost to process a prompt leading to longer inference and training time. Too many informations can overload the model, making it harder to focus on the most relevant piece of information. Lastly including redundant demonstrations can introduce noise and alter the model's understanding of the task"
      ],
      "metadata": {
        "id": "pWSPUvOX1Yr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fewshot Classification - Vector-based Retrieval\n",
        "\n",
        "Now, we want to improve demonstrayion by the vector representation of our sentence. This is close to what we did when we used RAG on wikipedia page. But here, we'll do it manually and step by step.\n",
        "\n",
        "To do so, we need to calculate the vector representation of our training dataset. To do this, we'll code a function that returns a vector for a given example. We'll use our LLM hidden states to do this. It's not optimal, but we won't have to load another model.\n",
        "\n",
        "First, look at the mistral architecture:\n",
        "\n",
        "```\n",
        "MistralForCausalLM(\n",
        "  (model): MistralModel(\n",
        "    (embed_tokens): Embedding(131072, 4096)\n",
        "    (layers): ModuleList(\n",
        "      (0-35): 36 x MistralDecoderLayer(\n",
        "        (self_attn): MistralSdpaAttention(\n",
        "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
        "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
        "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (rotary_emb): MistralRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): MistralMLP(\n",
        "          (gate_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
        "          (up_proj): Linear4bit(in_features=4096, out_features=12288, bias=False)\n",
        "          (down_proj): Linear4bit(in_features=12288, out_features=4096, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
        "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
        "  )\n",
        "  (lm_head): Linear(in_features=4096, out_features=131072, bias=False)\n",
        ")\n",
        "```\n",
        "There are 36 transformer layers and 1 language model (LM) layer. Each layer will take the following shape: [1, N_TOKENS, N_PARAMS]. We want to extract the vector of the last token from the last transformer. To do so:\n",
        "- Encode the sentence without any template. `tokenizer.encode(...)`\n",
        "- Use the `output_hidden_states` keyword of the llm forward function.\n",
        "- Select the last transformer layer (be careful, don't take the LM layer).\n",
        "- Select the last token.\n",
        "- Convert the vector to numpy `.to('cpu').float().numpy()` and return it.\n",
        "\n",
        "There is a cell below to test your code. The output should be:\n",
        "```\n",
        "# (4096,)\n",
        "# [ 4.59375    -9.          0.80078125 ...  0.890625   -0.20019531\n",
        " -0.62109375]\n",
        "```"
      ],
      "metadata": {
        "id": "ZvO1K4K_xstS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hidden_repr(text, llm=llm):\n",
        "    \"\"\"\n",
        "    Get the vector representation of the last token from the last transformer layer for a given text.\n",
        "    \"\"\"\n",
        "    tokenized = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = llm(**tokenized, output_hidden_states=True)\n",
        "    hidden_states = outputs.hidden_states[-2]\n",
        "    last_token_vector = hidden_states[0, -1, :]\n",
        "    return last_token_vector.to(\"cpu\").float().numpy()"
      ],
      "metadata": {
        "id": "mOeTRJZSfOsR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to get the hidden represation vector for all examples in the train and the test datasets.\n",
        "\n",
        "You should store the vector directly in the example dict: `example[\"vector\"] = ...`\n",
        "\n",
        "Both should take 3 - 5 mins to run."
      ],
      "metadata": {
        "id": "5F_KmHorGn4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Process train dataset\n",
        "for example in tqdm(train):  # tqdm allows you to track the progression of your loop.\n",
        "    text = example['text']\n",
        "    example['vector'] = get_hidden_repr(text, llm=llm)\n",
        "\n"
      ],
      "metadata": {
        "id": "PdyPc5OZ0CkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e8cb4d-7ecd-46a5-97a2-76063b1acf3b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [03:41<00:00,  1.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process test dataset\n",
        "for example in tqdm(test):  # tqdm allows you to track the progression of your loop.\n",
        "    text = example['text']\n",
        "    example['vector'] = get_hidden_repr(text, llm=llm)\n"
      ],
      "metadata": {
        "id": "9J0hBUiS6fCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8245b1-0ce7-42d0-ef61-40db948054a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:55<00:00,  1.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our vector representations. We want a function that compute the cosine similarity between 2 examples.\n",
        "\n",
        "- Use the function from sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
        "- Be careful, you have to reshape each vector to: [1, 4096]\n",
        "\n",
        "There is a cell below to test your code. The output should be:\n",
        "```\n",
        "# a . a = 1.0000019073486328\n",
        "# a . b = 0.930396318435669\n",
        "```"
      ],
      "metadata": {
        "id": "yHyFAsmR6QkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_similarity(example_a, example_b):\n",
        "    vector_a = example_a[\"vector\"].reshape(1, -1)\n",
        "    vector_b = example_b[\"vector\"].reshape(1, -1)\n",
        "    similarity = cosine_similarity(vector_a, vector_b)\n",
        "    return similarity[0][0]"
      ],
      "metadata": {
        "id": "zpLMGStl6Of9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last step, we want a function that retrieve the k more similar demonstrations of the train examples given a test example.\n",
        "\n",
        "There is a cell below to test your code. The output should be:\n",
        "```\n",
        "# surprise - i feel a strange gratitude for the hated israeli occupation of sinai that lasted from to for actually recognizing the importance of sinais history\n",
        "#  joy - i feel lucky that theyve chosen to share their lives with me\n",
        "\n",
        "joy - i feel our world then was a much more innocent place\n",
        "\n",
        "joy - i know he does the same thing for so many passersby i feel special truly welcome in his country\n",
        "\n",
        "joy - i do know that i tell some people if i feel that their question is sincere some of my sacred treasures\n",
        "\n",
        "anger - i feel appalled that i took advantage of my old friend s kindness\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "9p4LQCDw9JSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_k_similar_demo(example, k, train=train):\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import numpy as np\n",
        "\n",
        "    # Extract the vector from the input example\n",
        "    input_vector = example[\"vector\"].reshape(1, -1)\n",
        "\n",
        "    # Compute cosine similarities with all training examples\n",
        "    similarities = []\n",
        "    for train_example in train:\n",
        "        train_vector = train_example[\"vector\"].reshape(1, -1)\n",
        "        similarity = cosine_similarity(input_vector, train_vector)[0][0]\n",
        "        similarities.append((train_example, similarity))\n",
        "\n",
        "    # Sort by similarity in descending order\n",
        "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extract the top k examples\n",
        "    top_k_examples = [item[0] for item in similarities[:k]]\n",
        "\n",
        "    return top_k_examples\n"
      ],
      "metadata": {
        "id": "PDwXVZEc9Ieb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now apply the fewshot prompt on the full test dataset. You need report:\n",
        "- Accuracy (recall: number of correct answers divided by number of samples)\n",
        "- Report them for k=1 and k=5\n",
        "\n",
        "It should take 5 to 7 minutes to run.\n",
        "\n",
        "Your results should be:\n",
        "```\n",
        "##### k=1 #####\n",
        "Accuracy:  0.65\n",
        "##### k=5 #####\n",
        "Accuracy:  0.63\n",
        "```"
      ],
      "metadata": {
        "id": "WYh2wnruGdNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize variables for accuracy computation\n",
        "K = 1\n",
        "total_samples = len(test)\n",
        "results = []\n",
        "\n",
        "correct_predictions = 0\n",
        "\n",
        "print(f\"\\n\\n##### Evaluating for k={K} #####\")\n",
        "# Loop over the test dataset\n",
        "for example in tqdm(test):  # tqdm allows tracking the progression of the loop\n",
        "    # Get k most similar examples\n",
        "    similar_examples = get_k_similar_demo(example, K, train=train)\n",
        "\n",
        "    # Format the examples into the few-shot prompt\n",
        "    demos = format_demos(similar_examples)\n",
        "    prompt = fewshot_prompt.format(examples=demos, sentence=example[\"text\"])\n",
        "    prediction = parse_answer(answer)\n",
        "\n",
        "    results.append({\n",
        "        \"text\": example[\"text\"],\n",
        "        \"label\": example[\"label\"],\n",
        "        \"prediction\": prediction\n",
        "    })\n",
        "\n",
        "    if prediction == example[\"label\"]:\n",
        "        correct_predictions += 1\n",
        "    if prediction == \"\":\n",
        "        missing_answers += 1\n",
        "\n",
        "accuracy = correct_predictions / total_examples\n",
        "missing_answer_ratio = missing_answers / total_examples\n",
        "\n",
        "# Add statistics to the results\n",
        "results.append({\n",
        "    \"statistics\": {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"missing_answer_ratio\": missing_answer_ratio\n",
        "    }\n",
        "})\n",
        "\n",
        "with open(\"Vector_Classification_K1.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Missing answer ratio: {missing_answer_ratio}\")\n",
        "print(\"Results saved to Vector_Classification_K1.json\")\n"
      ],
      "metadata": {
        "id": "-d4UykmE9FqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a1021f-5f53-4af6-b765-308a1eed327f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "##### Evaluating for k=1 #####\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:27<00:00,  3.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.32\n",
            "Missing answer ratio: 0.0\n",
            "Results saved to Vector_Classification_K1.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize variables for accuracy computation\n",
        "K = 5\n",
        "total_samples = len(test)\n",
        "results = []\n",
        "\n",
        "correct_predictions = 0\n",
        "\n",
        "print(f\"\\n\\n##### Evaluating for k={K} #####\")\n",
        "# Loop over the test dataset\n",
        "for example in tqdm(test):  # tqdm allows tracking the progression of the loop\n",
        "    # Get k most similar examples\n",
        "    similar_examples = get_k_similar_demo(example, K, train=train)\n",
        "\n",
        "    # Format the examples into the few-shot prompt\n",
        "    demos = format_demos(similar_examples)\n",
        "    prompt = fewshot_prompt.format(examples=demos, sentence=example[\"text\"])\n",
        "    prediction = parse_answer(answer)\n",
        "\n",
        "    results.append({\n",
        "        \"text\": example[\"text\"],\n",
        "        \"label\": example[\"label\"],\n",
        "        \"prediction\": prediction\n",
        "    })\n",
        "\n",
        "    if prediction == example[\"label\"]:\n",
        "        correct_predictions += 1\n",
        "    if prediction == \"\":\n",
        "        missing_answers += 1\n",
        "\n",
        "accuracy = correct_predictions / total_examples\n",
        "missing_answer_ratio = missing_answers / total_examples\n",
        "\n",
        "# Add statistics to the results\n",
        "results.append({\n",
        "    \"statistics\": {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"missing_answer_ratio\": missing_answer_ratio\n",
        "    }\n",
        "})\n",
        "\n",
        "with open(\"Vector_Classification_K5.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Missing answer ratio: {missing_answer_ratio}\")\n",
        "print(\"Results saved to Vector_Classification_K5.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5WoUW3Z6vQs",
        "outputId": "ed9ea2b4-0e3c-4a3c-8786-2716e6a7e58a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "##### Evaluating for k=5 #####\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:25<00:00,  3.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.32\n",
            "Missing answer ratio: 0.0\n",
            "Results saved to Vector_Classification_K5.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question: What could be the main issue with this approach? How can it be mitigated?**"
      ],
      "metadata": {
        "id": "rBuOAMeypO1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector-based retrieval relies on semantic similarity, which might not be accurate enough for complex classifications. This can lead to irrelevant demonstrations being used.\n",
        "\n",
        "To solve this problem we could for example combine semantic similarity with other methods for a more robust retrieval. We could also fine-tune the LLM or use task-specific embeddings for more accurate representations."
      ],
      "metadata": {
        "id": "Oi6Quiuf68wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Constrained Decoding\n",
        "\n",
        "Last exercise, we will use the `outlines` package to do constrained generation. This main idea is to guide the generation of the LLM to get the good output formats.\n",
        "\n",
        "We will use the choices module. Here is the documentation: https://dottxt-ai.github.io/outlines/latest/reference/generation/choices/\n",
        "\n",
        "There is an example below on how to use it on 1 example. We let you apply this methods to the test dataset. You need report:\n",
        "- Accuracy (recall: number of correct answers divided by number of samples)\n",
        "- Ratio of missing answer (i.e \"E.\" answer)\n",
        "- Report them for k=1 and k=5\n",
        "\n",
        "It should take 3 to 5 minutes to run.\n",
        "\n",
        "Your results should be:\n",
        "```\n",
        "Accuracy:  0.38\n",
        "```"
      ],
      "metadata": {
        "id": "AzotKcwQKy44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from outlines import models, generate\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "model = models.Transformers(llm, tokenizer)\n",
        "\n",
        "labels = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
        "generator = generate.choice(model, labels)\n",
        "\n",
        "results = []\n",
        "correct_predictions = 0\n",
        "missing_answers = 0\n",
        "total_examples = len(test)\n",
        "\n",
        "for example in tqdm(test):\n",
        "\n",
        "    prompt = zeroshot_prompt.format(sentence=example[\"text\"])\n",
        "    answer = generator(prompt)\n",
        "\n",
        "    results.append({\n",
        "        \"text\": example[\"text\"],\n",
        "        \"label\": example[\"label\"],\n",
        "        \"prediction\": prediction\n",
        "    })\n",
        "\n",
        "    if prediction == example[\"label\"]:\n",
        "        correct_predictions += 1\n",
        "    if prediction == \"\":\n",
        "        missing_answers += 1\n",
        "\n",
        "accuracy = correct_predictions / total_examples\n",
        "missing_answer_ratio = missing_answers / total_examples\n",
        "\n",
        "# Add statistics to the results\n",
        "results.append({\n",
        "    \"statistics\": {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"missing_answer_ratio\": missing_answer_ratio\n",
        "    }\n",
        "})\n",
        "\n",
        "with open(\"Outines_Classification.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Missing answer ratio: {missing_answer_ratio}\")\n",
        "print(\"Results saved to Outlines_Classification.json\")"
      ],
      "metadata": {
        "id": "bZTiTM04__PE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d29bbb-12a2-4e24-f113-0fc9065a15ef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:51<00:00,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.32\n",
            "Missing answer ratio: 0.0\n",
            "Results saved to Outlines_Classification.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question: Now that you've used all these solutions, when should you use zeroshot? when should you use fewshot? when should you use constrained decoding?**"
      ],
      "metadata": {
        "id": "0BEiQQm2p9aG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zeroshot**: Use for simple tasks, limited data, or quick prototyping.\n",
        "\n",
        "**Fewshot**: Use to boost accuracy over zeroshot, handle complex tasks, or when you have data for demonstrations.\n",
        "\n",
        "**Constrained Decoding**: Use to control output format, reduce hallucinations, or integrate with downstream tasks."
      ],
      "metadata": {
        "id": "FyIW_7FG4_L3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus\n",
        "\n",
        "Try to use differents modules of `outlines` like json, pydantic or regex ...\n",
        "\n",
        "Compare this results with previous ones !"
      ],
      "metadata": {
        "id": "1H_HqjALp3pG"
      }
    }
  ]
}